{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJJCNEYMp64R"
   },
   "source": [
    "# Experimental design\n",
    "\n",
    "_Alex I. Malz (LINCC@CMU)_\n",
    "\n",
    "_LSSTC DSFP #16_\n",
    "\n",
    "As this notebook was adapted from a data challenge elsewhere, if you use anything from here in a publication, cite [the repo it came from](https://github.com/aimalz/qtc2021) and acknowledge \"[Quarks to Cosmos with AI](https://events.mcs.cmu.edu/qtc2021/), a conference supported by the NSF AI Institute: Physics of the Future, NSF PHY-2020295.\"\n",
    "\n",
    "## Goals\n",
    "\n",
    "In the course of your research, you're very likely going to find yourself choosing between multiple ways of completing a task.\n",
    "When the task is a step in an analysis pipeline, the choice you make constitutes an inferential model.\n",
    "In asking the question, \"Which model is best?\" we are acting as experimenters, and the metric is the way we decide which to choose.\n",
    "Therefore, the metric must correspond to what we actually want!\n",
    "\n",
    "So, in this notebook, you'll be provided with some choices of photo-$z$ estimation models, priors to feed the model, options for data upon which to test the models, and a variety of metrics to evaluate the estimates.\n",
    "Your objective is to uncover the unstated assumptions, probe the limiting cases, and overall figure out how to break the experiment.\n",
    "\n",
    "\n",
    "## Structure\n",
    "\n",
    "This notebook contains some pieces necessary to build up an experiment in the context of photometric redshifts.\n",
    "You get to make choices about the photo-z estimation model to test, the priors to feed that model, and the metrics by which to evaluate the resulting posteriors.\n",
    "The goal is to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O19PzXur-GF"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "More and more machine learning (ML) methods, particularly in Bayesian deep learning, yield uncertainties $\\hat{p}(y | x_{i})$ of target parameters $y$ given observed random variables $x_{i}$, rather than just point estimates $\\hat{y}_{i}$.\n",
    "Though rarely framed as such, these uncertainties are _posteriors_, and there's more than meets the eye to what lies on the righthand side of the conditional.\n",
    "\n",
    "Really, an estimated posterior should be writen as $p(y | x_{i}, \\pi, M)$ for prior $\\pi$ (which is training data $\\{y_{n}, x_{n}\\}_{N}$ for a machine learning approach and takes other forms for physics-informed models) and algorithm (model) $M$.\n",
    "Though the dependence on prior information is straightforward, the dependence on the algorithm, meaning the estimation model and its implementation, is subtle.\n",
    "However, if it were not there, then every model with the same prior information would yield identical estimated uncertainties, which is not observed.\n",
    "While the $\\pi$ (e.g. training set $\\{y_{n}, x_{n}\\}_{N}$) is equivalent to an _explicit prior_, the algorithm $M$ must be considered an _implicit prior_, in that we don't know how to write down how it projects onto the space of data.\n",
    "\n",
    "Another way of looking at estimated posteriors is in terms of the type of uncertainty encompassed by each term on the righthand side of the conditional.\n",
    "For a noisy measurement or a stochastic generative process, the random variable $x_{i} \\sim p(x | y_{i})$ represents the _aleatoric uncertainty_, the uncertainty inherent to the data.\n",
    "However, the training set $\\{y_{n}, x_{n}\\}_{N}$ and implemented etimation model $M$ could potentially be improved to yield a better estimate and thus constitute sources of _epistemic uncertainty_, the uncertainty due to an imperfect model.\n",
    "In physics, we want to learn the aleatoric uncertainty $p(x | y_{i})$, which we can't get from the estimated posteriors $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, M)$ in hand without knowing $p(y | \\{y_{n}, x_{n}\\}_{N}, M)$.\n",
    "\n",
    "Meanwhile, assessments of the performance of estimated $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, M)$ are almost always made by comparison to known $y_{i}$, leaving unanswered the question of how well the estimator approximates $p(y | x_{i})$.\n",
    "Why?\n",
    "The problem is that $p(y | x_{i})$ is not necessarily known, certainly not for observed $y_{i}$ measured in nature, but also generally for simulated $y_{i}$, at least in astrophysics.\n",
    "To avoid the pitfalls of the LSST-DESC PZ DC1 experiment, we want to have those true posteriors, so we have to design an experiment that makes those accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW-T6Zq70MwL"
   },
   "source": [
    "## Context: Photometric redshifts\n",
    "\n",
    "[Photo-$z$s](https://en.wikipedia.org/wiki/Photometric_redshift) provide an excellent testbed for addressing these issues but require some introduction.\n",
    "There isn't a definitive primer, but here are several overviews that are informative, if a bit dry.\n",
    "- [basic intro from Rubin Observatory](https://www.lsst.org/science/dark-energy/photometric-redshift)\n",
    "- [old overview covering classic concepts](https://ned.ipac.caltech.edu/level5/Glossary/Essay_photredshifts.html)\n",
    "- [recent review of estimation methods](https://arxiv.org/abs/1805.12574)\n",
    "If these don't answer your questions, Chapter 0 of [my thesis](https://zenodo.org/record/3973536) might, or, better yet, [the slides from my defense](https://github.com/aimalz/ship-of-theses/tree/master/presentation) make for a better tl;dr.\n",
    "\n",
    "One reason photo-$z$s are perfect for a data challenge is that they are simple enough to obtain $p(y | x_{i})$ along the way to generating a sample of $(y_{i}, x_{i})$ pairs.\n",
    "Here, the target variable $y \\to z$ is redshift, a scalar, and the data $x \\to \\vec{d} = (u, g, r, i, z, y)$, or some trivial function thereof, is a vector of length $<10$ observed [photometric magnitudes](https://en.wikipedia.org/wiki/Magnitude_(astronomy)) of galaxies through [broadband optical filters](https://en.wikipedia.org/wiki/Photometric_system) (which I somewhat arbitrarily choose to be those of [the Vera C. Rubin Observatory](https://www.lsst.org/)).\n",
    "Because of the extremely low dimensionality of the problem, we can forward model not just individual pairs $(z_{i}, \\vec{d}_{i})$ but the entire joint probability space $p(z, \\vec{d})$, thereby obtaining $p(y | x_{i})$ for every $(z_{i}, \\vec{d}_{i})$.\n",
    "That doesn't mean it's trivial to do so -- most of this tutorial concerns that forward-modeling procedure -- but it is possible.\n",
    "\n",
    "Another reason photo-$z$s are perfect for this data challenge is that comprehensive uncertainty quantification for galaxy redshifts in the absence of spectroscopy are crucial for the Legacy Survey of Space and Time (LSST), an upcoming photometric survey on the Rubin Observatory.\n",
    "This data challenge has the ulterior motive of strengthening the cosmology analysis pipeline of LSST's Dark Energy Science Collaboration (DESC).\n",
    "This tutorial makes use of two pieces of software DESC members are publicly developing right now, [RAIL](https://github.com/LSSTDESC/RAIL) and [qp](https://github.com/LSSTDESC/qp), whose functionality will be introduced where relevant, along with other code dependencies.\n",
    "Participants need not use either in their responses to the challenge questions, but the development team welcomes feedback from potential users, contributions to the codebase that could result from this data challenge, and new team members, DESC membership not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIOag5iVqZPW"
   },
   "source": [
    "# Tutorial and challenge prompts\n",
    "\n",
    "Finally, without more ado, we set the stage with a tutorial that:\n",
    "1. creates realistically complex mock photo-$z$ posteriors, redshifts, and photometry to define training and test sets;\n",
    "2. estimates photo-$z$ posteriors of the test set given the training set using ML;\n",
    "3. quantifies how closely the estimated posteriors approximate the true ones.\n",
    "\n",
    "_The three-pronged structure of this tutorial is inspired by that of RAIL, which has subpackages corresponding to each of the enumerated parts of the tutorial: `rail.creation`, `rail.estimation`, and `rail.evaluation`._\n",
    "\n",
    "The challenge comes from building on the tutorial content to conduct a self-guided investigation of the open questions.\n",
    "To get the most out of this opportunity, think of this tutorial as a lab manual that presents an experimental procedure to answer each question and includes an example of a possible solution, then invites participants to \"riff\" on those solutions to devise and implement their own.\n",
    "The greatest opportunities for such investigations can be found under the headings **\"Your turn!\"** throughout the tutorial, but there are certainly more interesting directions to go in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JituVGNVZ7bg"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2llNhgut6jM",
    "outputId": "41d9f8cd-1b90-4e45-91f6-6d9c2f6a4d75"
   },
   "outputs": [],
   "source": [
    "# !pip install cde-diagnostics\n",
    "# !pip install cdetools\n",
    "# !pip install corner\n",
    "# !pip install FlexCode\n",
    "# !pip install pzflow\n",
    "# !pip install sklearn\n",
    "# !pip install xgboost==0.90\n",
    "# !pip install qp-prob\n",
    "\n",
    "# !git clone https://github.com/COINtoolbox/photoz_catalogues.git .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNhx0u1vszZN"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHM0pdtvLdG2",
    "outputId": "fa437d28-233f-400c-8d06-040bfec6e2fd"
   },
   "outputs": [],
   "source": [
    "import cdetools\n",
    "import flexcode\n",
    "import pzflow\n",
    "import qp\n",
    "# import rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZs_a1wy0EbC"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpl_patches\n",
    "mpl.rc('text', usetex=False)\n",
    "\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9imLKSxZ_F1"
   },
   "source": [
    "## 1. Generating mock data, including true photo-$z$ posteriors\n",
    "\n",
    "This is the most complex of the parts of the tutorial, which should leave readers with a sense of why this kind of experiment has not been done for higher-dimensional data.\n",
    "There are three main steps:\n",
    "1. Prepare and explore data to use as the basis for a realistically complex generative model.\n",
    "2. Emulate a few realistically complex $p(z, \\vec{d})$ probability spaces using input data.\n",
    "3. Sample $p(z, \\vec{d})$ and $p'(z, \\vec{d})$ to produce photometric data sets to use as explicit prior information and on which to estimate redshift posteriors, with known $z$, $\\vec{d}$, and $p(z | \\vec{d})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pexhHnZsoNq"
   },
   "source": [
    "### Data\n",
    "\n",
    "Though we're going to forward-model mock data to experiment on, we want it to be _realistically complex_, meaning it shares the physical degeneracies and systematic errors that would be present in a real data set.\n",
    "The [Happy/Teddy data sets](https://github.com/COINtoolbox/photoz_catalogues) (see [Beck, et al 2017](https://arxiv.org/abs/1701.08748) for full release notes) are curated subsamples of the [Sloan Digital Sky Survey (SDSS) Data Release (DR) 12](https://www.sdss.org/dr12/), a spectroscopic survey with high-fidelity redshift measurements, and were created by the [Cosmostatistics Initiative (COIN)](https://cosmostatistics-initiative.org/).\n",
    "The data sets are defined to emulate the kinds of differences in observational properties of galaxies with measured spectroscopic redshifts and those for which only photometry is available, and they were created with the goal of determining the impact of imbalance between training, validation, and test sets for photo-$z$ point estimation.\n",
    "They are thus an appropriate starting point for creating a realistically complex model of the joint probability space $p(z, \\vec{d})$.\n",
    "\n",
    "Of course there are plenty of other potential data sets available, including those that are simulated, which may be advantageous for the potential to estimate other galaxy properties such as stellar mass and star formation rate that are known in a simulation but not directly measurable with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71MdMCuXn16X"
   },
   "outputs": [],
   "source": [
    "happy_path = '../photoz_catalogues/Happy/happy_'\n",
    "header = pd.read_csv(happy_path+'A', delim_whitespace=True, nrows=0).columns[1:]\n",
    "teddy_path = '../photoz_catalogues/Teddy/teddy_'\n",
    "\n",
    "happy, teddy = {}, {}\n",
    "for lett in ['A', 'B', 'C', 'D']:\n",
    "    happy[lett] = pd.read_csv(happy_path+lett, delim_whitespace=True, header=None, skiprows=1, names=header)\n",
    "    happy[lett] = happy[lett].rename(columns={'z_spec':'redshift', 'mag_r': 'r'})[['redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "#     happy[lett]\n",
    "    teddy[lett] = pd.read_csv(teddy_path+lett, delim_whitespace=True, header=None, skiprows=7, names=header)\n",
    "    teddy[lett] = teddy[lett].rename(columns={'z_spec':'redshift', 'mag_r': 'r'})[['redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "#     teddy[lett]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "TV_VxG8CtoDW",
    "outputId": "50e07d68-aec6-4b18-ab5c-eb9fbeb0cd66"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for j, col in enumerate(['redshift', 'r']):\n",
    "    for i, lett in enumerate(['A', 'B', 'C', 'D']):\n",
    "        ax[j][0].hist(happy[lett][col], alpha=0.25, bins=100, density=False, label='happy_'+lett)\n",
    "        ax[j][1].hist(teddy[lett][col], alpha=0.25, bins=100, density=False, label='teddy_'+lett)\n",
    "        ax[j][0].set_xlabel(col)\n",
    "        ax[j][0].legend()\n",
    "        ax[j][1].set_xlabel(col)\n",
    "        ax[j][1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience later on, we can safely cut off the tiny fraction of outliers in redshift and $r$-band magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WutxjLl4YvLG"
   },
   "outputs": [],
   "source": [
    "z_min, z_max = 0., 1.5\n",
    "r_min, r_max = 10., 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9YMBF_NsekS"
   },
   "outputs": [],
   "source": [
    "colorcycle = 'rbgcmy'\n",
    "fig = corner.corner(happy['A'][['u-g', 'g-r', 'r-i', 'i-z']], color='k', alpha=0.25)\n",
    "for i, lett in enumerate(['B', 'C', 'D']):\n",
    "    corner.corner(happy[lett][['u-g', 'g-r', 'r-i', 'i-z']], fig=fig, color=colorcycle[i], alpha=0.25)\n",
    "    \n",
    "fig = corner.corner(teddy['A'][['u-g', 'g-r', 'r-i', 'i-z']], color='k', alpha=0.25)\n",
    "for i, lett in enumerate(['B', 'C', 'D']):\n",
    "    corner.corner(teddy[lett][['u-g', 'g-r', 'r-i', 'i-z']], fig=fig, color=colorcycle[i], alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuzPDc5WsrUK"
   },
   "source": [
    "### Emulating $p(z, \\vec{d})$ from input data with `pzflow`\n",
    "\n",
    "[`pzflow`](https://github.com/jfcrenshaw/pzflow) is a package for making normalizing flows from sets of redshifts and photometry in order to estimate or otherwise model photo-$z$ posteriors.\n",
    "We'll use it to make a normalizing flow that will serve as the model for $p(z, \\vec{d})$.\n",
    "\n",
    "_This content is adapted from pzflow's [demo](https://github.com/jfcrenshaw/pzflow/blob/main/docs/tutorials/intro.ipynb), written by John Franklin Crenshaw (UW)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yW3ONw4q9pw"
   },
   "outputs": [],
   "source": [
    "from pzflow import Flow\n",
    "from pzflow.bijectors import Chain, ColorTransform, InvSoftplus, StandardScaler, RollingSplineCoupling, ShiftBounds\n",
    "from pzflow.examples import get_galaxy_data\n",
    "from pzflow.distributions import Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a demonstration of how pzflow makes a model of $p(z, \\vec{d})$ from its own demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_galaxy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxmVog1_kTqu"
   },
   "outputs": [],
   "source": [
    "data = get_galaxy_data()\n",
    "\n",
    "# restrict to Happy/Teddy range for coverage in demo\n",
    "data = data[(data['redshift'] > z_min) & (data['redshift'] < z_max) & (data['r'] > r_min) & (data['r'] < r_max)]\n",
    "\n",
    "# normalize\n",
    "data = data\n",
    "\n",
    "# use fewer bands to be able to compare with Happy/Teddy\n",
    "data = data[['redshift', 'u', 'g', 'r', 'i', 'z']]\n",
    "\n",
    "# convert magnitudes to a reference magnitude and colors\n",
    "data['u-g'] = data['u'] - data['g']\n",
    "data['g-r'] = data['g'] - data['r']\n",
    "data['r-i'] = data['r'] - data['i']\n",
    "data['i-z'] = data['i'] - data['z']\n",
    "\n",
    "# save the new set\n",
    "data = data[['redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "4KdppeUD5t7l",
    "outputId": "905f9ff8-f4b3-4e58-db71-5c961a9c4c40"
   },
   "outputs": [],
   "source": [
    "# set the inverse softplus parameters, estimated\n",
    "# to ensure that sampled redshifts are positive\n",
    "column_idx = 0\n",
    "\n",
    "n_epoch = 30\n",
    "\n",
    "mins = [z_min, r_min, -1, -1, -1, -1]\n",
    "maxs = [z_max, r_max, 5, 5, 5, 5]\n",
    "\n",
    "# construct our bijector\n",
    "# by chaining all these layers\n",
    "bijector = Chain(\n",
    "    ShiftBounds(mins, maxs, B=5),\n",
    "    RollingSplineCoupling(nlayers=6)\n",
    ")\n",
    "\n",
    "\n",
    "flow = Flow(data.columns, bijector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may be slow (~minutes) if you don't have a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = flow.train(data, epochs=n_epoch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "4KdppeUD5t7l",
    "outputId": "905f9ff8-f4b3-4e58-db71-5c961a9c4c40"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.save('default_flow.pkl')\n",
    "flow = Flow(file='default_flow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose a redshift grid upon which to evaluate the posterior PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNl-Q4YqXaxc"
   },
   "outputs": [],
   "source": [
    "granularity = 100\n",
    "grid = np.linspace(z_min, z_max, granularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check one of them before making more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "qS_Zs9Vqbs3x",
    "outputId": "85c91abd-38e8-4ac9-b73a-e8b5bc9b5a2e"
   },
   "outputs": [],
   "source": [
    "chosen = 999\n",
    "\n",
    "galaxy = data.iloc[[chosen]]\n",
    "pdf = flow.posterior(galaxy, column=\"redshift\", grid=grid)\n",
    "\n",
    "plt.plot(grid, pdf[0], label='Posterior')\n",
    "plt.axvline(galaxy['redshift'].values[0], 0, 1, c='C3', label='True redshift')\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the model of $p(z, data)$.\n",
    "(If you run out of memory, take fewer samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "OGrTo5J4TKuQ",
    "outputId": "563c1549-0340-4124-e5ab-cefb70f7e8a6"
   },
   "outputs": [],
   "source": [
    "samples = flow.sample(10000, conditions=data[:5000], seed=0)\n",
    "plt.hist(data['redshift'], range=(0, 2.5), bins=40, histtype='step', label='data', density=True)\n",
    "plt.hist(samples['redshift'], range=(0, 2.5), bins=40, histtype='step', label='samples', density=True)\n",
    "plt.xlabel('z')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwGykJnbmqys"
   },
   "outputs": [],
   "source": [
    "z = samples['redshift']\n",
    "z.to_csv('test_set_redshifts.csv')\n",
    "\n",
    "phot = samples[['r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "phot.to_csv('test_set_photometry.csv')\n",
    "\n",
    "posteriors = flow.posterior(samples, column=\"redshift\", grid=grid)\n",
    "with open('test_set_posteriors.csv', 'wb') as fn:\n",
    "    jnp.save(fn, posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyecUsXrJJ7R"
   },
   "source": [
    "We can do this procedure for all the Happy/Teddy samples so we can experiment with them later.\n",
    "It's slow without using GPU, but it only needs to be done once.\n",
    "The model files are also provided in the repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvYgIMnWuq9b",
    "outputId": "f51a1687-653a-40d0-db10-bd5545065208"
   },
   "outputs": [],
   "source": [
    "# full_data = {'happy': happy, 'teddy': teddy}\n",
    "# n_out = 1000\n",
    "\n",
    "# for name, dat in full_data.items():\n",
    "#     for lett in ['A', 'B', 'C', 'D']:\n",
    "#         print(name+lett)\n",
    "#         print(dat[lett].columns)\n",
    "#         print(len(dat[lett]))\n",
    "        \n",
    "#         flow = Flow(dat[lett].columns, bijector)\n",
    "\n",
    "#         losses = flow.train(dat[lett], epochs=n_epoch, verbose=True)\n",
    "#         flow.save(name+lett+'flow.pkl')\n",
    "#         print('done with '+name+lett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvYgIMnWuq9b",
    "outputId": "f51a1687-653a-40d0-db10-bd5545065208"
   },
   "outputs": [],
   "source": [
    "for name, dat in full_data.items():\n",
    "    for lett in ['A', 'B', 'C', 'D']:     \n",
    "        print(name+lett)\n",
    "        flow = Flow(file=name+lett+'flow.pkl')\n",
    "\n",
    "        samples = flow.sample(10000, conditions=dat[lett].sample(n_out), seed=0)\n",
    "\n",
    "        z = samples['redshift']\n",
    "        z.to_csv(name+lett+'redshifts.csv', index=False)\n",
    "\n",
    "        phot = samples[['r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
    "        phot.to_csv(name+lett+'photometry.csv', index=False)\n",
    "\n",
    "        posteriors = flow.posterior(samples, column=\"redshift\", grid=grid)\n",
    "        with open(name+lett+'posteriors.csv', 'wb') as fn:\n",
    "            jnp.save(fn, posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Try to \"degrade\" the data to introduce degeneracies, inconsistencies, and nonrepresentativity that could trip up an estimator with or without being missed by a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xpQw49TaR03"
   },
   "source": [
    "## 2. Estimating photo-z posterior PDFs\n",
    "\n",
    "There are many estimators of photo-$z$ posteriors, and many of those are compared to one another in [Schmidt & Malz, et al. 2020](https://arxiv.org/abs/2001.03621). \n",
    "Of the tested estimators, including ML and non-ML methods, the most promising was [`FlexCode`](https://github.com/tpospisi/FlexCode) ([Izbicki & Lee, 2017](https://arxiv.org/abs/1704.08095)), which also happens to be one of the easiest to install and apply, so we'll demonstrate it as an example of an estimator of photo-$z$ posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZsmK6L8v9OO"
   },
   "source": [
    "### Estimating photo-$z$ posterior PDFs with FlexCode\n",
    "\n",
    "A demonstration of `FlexCode` in the context of photo-$z$s can be found in [Dalmasso, et al 2019](https://arxiv.org/abs/1908.11523), with demos in `R` published on [GitHub](https://github.com/Mr8ND/cdetools_applications).\n",
    "We'll demonstrate it on the pzflow-based samples generated from the Happy/Teddy data sets.\n",
    "**TODO: save the outputs for convenience**\n",
    "\n",
    "_This content is adapted from FlexCode's [Teddy tutorial](https://github.com/tpospisi/FlexCode/blob/master/tutorial/Flexcode-tutorial-teddy.ipynb), written by Nic Dalmasso (CMU)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGZ8i_opZiFY"
   },
   "outputs": [],
   "source": [
    "n_grid = granularity\n",
    "\n",
    "# Select regression method\n",
    "from flexcode.regression_models import NN\n",
    "\n",
    "# Parameters\n",
    "basis_system = \"cosine\"  # Basis system\n",
    "max_basis = 31           # Maximum number of basis. If the model is not tuned,\n",
    "                         # max_basis is set as number of basis\n",
    "\n",
    "n_estimators = 100\n",
    "criterion = 'mse'\n",
    "max_depth = 5\n",
    "    \n",
    "# Regression Parameters \n",
    "# If a list is passed for any parameter automatic 5-fold CV is used to\n",
    "# determine the best parameter combination.\n",
    "params = {\"k\": 20}#[5, 10, 15, 20]}       # A dictionary with method-specific regression parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udbXVbiDZwie"
   },
   "source": [
    "Let's try first with a representative training/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb0yIUIjXSJm"
   },
   "outputs": [],
   "source": [
    "x_orig = pd.read_csv('test_set_photometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
    "y_orig = pd.read_csv('test_set_redshifts.csv')[['redshift']].to_numpy()\n",
    "posteriors_orig = pd.DataFrame(np.load('test_set_posteriors.csv')).to_numpy()\n",
    "\n",
    "# n_samp = 10000\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, posteriors_train, posteriors_test = train_test_split(x_orig, y_orig, posteriors_orig, \n",
    "                                                                                       train_size=2000, random_state=42)\n",
    "x_train, x_validation, y_train, y_validation, posteriors_train, posteriors_validation = train_test_split(x_train, y_train, posteriors_train, \n",
    "                                                                                                         train_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lo6J8wY7pb1U"
   },
   "outputs": [],
   "source": [
    "# Parameterize model\n",
    "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
    "\n",
    "# Fit model - this will also choose the optimal number of neighbors `k`\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Tune model - Select the best number of basis\n",
    "model.tune(x_validation, y_validation)\n",
    "\n",
    "# Predict new densities on grid\n",
    "cde_test, y_grid = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine one of the photo-$z$ posteriors estimated with the perfectly representative training/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "sQ8plrbNCoLn",
    "outputId": "7505e7b5-bccc-403c-8d70-f37ccd0e2794"
   },
   "outputs": [],
   "source": [
    "chosen = 9\n",
    "\n",
    "plt.plot(y_grid, cde_test[chosen], label='Estimated posterior')\n",
    "plt.plot(grid, posteriors_test[chosen], label='True posterior')\n",
    "plt.axvline(y_test[chosen], 0, 1, c='C3', label='True redshift')\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAF62sfOawmd"
   },
   "source": [
    "It looks pretty good!\n",
    "Now let's try training and validating with some Happy/Teddy data but estimating posteriors on the test set from the pzflow demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWljb73yRRlT"
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('teddyAredshifts.csv')['redshift'].to_numpy()\n",
    "x_train = pd.read_csv('teddyAphotometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
    "\n",
    "y_validation = pd.read_csv('teddyBredshifts.csv')['redshift'].to_numpy()\n",
    "x_validation = pd.read_csv('teddyBphotometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmNhNgiIZp12"
   },
   "outputs": [],
   "source": [
    "# Parameterize model\n",
    "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
    "\n",
    "# Fit model - this will also choose the optimal number of neighbors `k`\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# # # Tune model - Select the best number of basis\n",
    "model.tune(x_validation, y_validation)\n",
    "\n",
    "# # Predict new densities on grid\n",
    "cde_test_bias, y_grid_bias = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwnYgZL6aWUP"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_grid_bias, cde_test_bias[chosen], label='Estimated posterior (biased training set)')\n",
    "plt.plot(y_grid, cde_test[chosen], label='Estimated posterior (unbiased training set)')\n",
    "plt.plot(grid, posteriors_test[chosen], label='True posterior')\n",
    "plt.axvline(y_test[chosen], 0, 1, c='C3', label='True redshift')\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the biased training and validation sets worsen the estimated posterior PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a\n",
    "\n",
    "Try retraining `FlexCode` and estimating photo-$z$ posteriors on different combinations of the data sets so you have some options to compare.\n",
    "Hypothesize which test cases will perform well and which will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: Estimating photo-$z$ posteriors with `trainZ`\n",
    "\n",
    "`trainZ` was the \"winner\" of the DESC PZ DC1 experiment by traditional metrics.\n",
    "Try implementing it here and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c: Estimating photo-$z$ posteriors with `scikit-learn`\n",
    "\n",
    "`scikit-learn` has a lot of prepackaged estimation methods you can try, just so you have more options for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg4SzDIYiTGf"
   },
   "source": [
    "## 3. Evaluating the performance of estimated photo-$z$ posterior PDFs\n",
    "\n",
    "Once we have estimated photo-$z$ posterior PDFs, we need a way to determine if they're actually any good.\n",
    "Since the tutorial has only one method but multiple training/validation sets, that's all we can compare for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo-$z$ point estimates (summary statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3a**\n",
    "\n",
    "Implement a few scalar summary statistics from the PDFs, e.g. the mean, median, and mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3b**\n",
    "\n",
    "Make a scatterplot of the point estimates versus the true redshifts.\n",
    "Include marginal histograms of the distributions of true and point-estimate redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics of photo-$z$ point estimates and true redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3c**\n",
    "\n",
    "The _bias_ is usually defined as $\\frac{\\hat{z} - z_{true}}{1+z_{true}}$, although [Graham+18](http://stacks.iop.org/1538-3881/155/i=1/a=1) defines a \"robust\" version as $\\frac{\\hat{z} - z_{true}}{1+\\hat{z}}$.\n",
    "Implement these, evaluate them on your test cases, and plot the results as histograms or averages in redshift bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3d**\n",
    "\n",
    "The _scatter_ is usually defined as $\\frac{(\\hat{z} - z_{true})^{2}}{1+z_{true}}$.\n",
    "Implement it, evaluate it on your test cases, and plot the results as histograms or averages in redshift bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3e**\n",
    "\n",
    "The catastrophic outlier rate is defined as the fraction of galaxies for which $|\\hat{z} - z_{true}| > 3 * \\sigma_{z}$.\n",
    "Implement it, calculate it on your test cases, and plot the results as histograms or averages in redshift bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYd_evyhw3pC"
   },
   "source": [
    "### Metrics of estimated photo-$z$ posteriors and true redshifts\n",
    "\n",
    "First, let's try out a couple metrics of estimated photo-$z$ posteriors that do not require knowledge of the true photo-$z$ posteriors.\n",
    "There's additional functionality for the case of having true redshifts but not true posteriors in [cdetools](https://github.com/tpospisi/cdetools) and [cde-diagnostics](https://github.com/zhao-david/CDE-diagnostics), but this should give a general idea.\n",
    "\n",
    "**Problem 3f**\n",
    "The implementations below are evaluated on just a subset of the possible combinations.\n",
    "Evaluate them on the test cases you considered and compare them to the point estimate metrics.\n",
    "Do they tell you the same thing or something different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L53A-iBubIvZ"
   },
   "outputs": [],
   "source": [
    "from cdetools import cde_loss, cdf_coverage, hpd_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sVDo_DLgGit"
   },
   "source": [
    "The Probability Integral Transform (PIT) is defined as \n",
    "\\begin{equation}\n",
    "PIT = \\int_{-\\infty}^{z_{true}} p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) dz .\n",
    "\\end{equation}\n",
    "A histogram of PIT values is commonly used to assess how consistent a population of photo-$z$ PDFs are with the true redshifts.\n",
    "Ideally, it would be a uniform distribution, meaning N% of galaxies have their true redshift within the Nth percentile of their estimated photo-$z$ posterior PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "uCWuAkGWk2ik",
    "outputId": "084df993-edb1-48f2-c743-5f4514e30217"
   },
   "outputs": [],
   "source": [
    "pit_values = cdf_coverage.cdf_coverage(cde_test, y_grid, y_test)\n",
    "pit_values_bias = cdf_coverage.cdf_coverage(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "plt.hist(pit_values, alpha=0.5, bins=100, label='representative', density=True)\n",
    "plt.hist(pit_values_bias, alpha=0.5, bins=100, label='biased', density=True)\n",
    "# plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.xlabel('PIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxtSTGtlrbq"
   },
   "source": [
    "The Highest Predictive Density (HPD) \n",
    "\\begin{equation}\n",
    "HPD = \\int_{z': p(z' | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) \\geq p(z | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)} p(z' | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) dz\n",
    "\\end{equation}\n",
    "is like the area of the PDF where it exceeds a given value.\n",
    "Over a population, it would ideally be flat, like the PIT.\n",
    "[A talk by David Zhao (CMU)](https://drive.google.com/file/d/1uvPtK_RcTUHEwt0ZYld41VKEPHnehWbN/view) has a lovely visualization of the HPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84WUFQ8rfVtZ"
   },
   "outputs": [],
   "source": [
    "hpd_cov = hpd_coverage.hpd_coverage(cde_test, y_grid, y_test)\n",
    "hpd_cov_bias = hpd_coverage.hpd_coverage(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "plt.hist(hpd_cov, alpha=0.5, bins=100, label='representative', density=True)\n",
    "plt.hist(hpd_cov_bias, alpha=0.5, bins=100, label='biased', density=True)\n",
    "# plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.xlabel('HPD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgkvMM8enL48"
   },
   "source": [
    "The CDE loss \n",
    "\\begin{equation}\n",
    "\\hat{L} = \\frac{1}{K} \\sum_{i=1}^{K} \\int \\left(p(z | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)\\right)^{2} dz - \\frac{2}{K} \\sum_{i=1}^{K} p(z_{i} | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)\n",
    "\\end{equation}\n",
    "approximates the true posterior from the estimated posterior evaluated at the true redshift.\n",
    "It's explained quite well in [a talk by Nic Dalmasso (CMU)](https://www.dropbox.com/s/2r4tl4qv0iyqo9b/STAMPS_LSST_CDE_Tools_Presentation.pdf?dl=0).\n",
    "A lower value indicates a better estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQq6sv7JV4bH",
    "outputId": "3f0d78c8-9e80-4cc9-c353-ff96db968392"
   },
   "outputs": [],
   "source": [
    "print(cde_loss.cde_loss(cde_test, y_grid, y_test))\n",
    "print(cde_loss.cde_loss(cde_test_bias, y_grid_bias, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbTHU-y6xNKX"
   },
   "source": [
    "### Comparison of estimated and true photo-$z$ posterior PDFs\n",
    "\n",
    "There are two categories of metrics of approximated and true PDF: \n",
    "those that either rely upon or force the normalization condition $\\int p(z) dz = 1$and those that evaluate differences between arbitrary functions.\n",
    "`qp` [(Malz, et al 2018)](https://arxiv.org/abs/1806.00014) is a package for manipulating univariate PDFs under many parameterizations and includes a few comparison metrics.\n",
    "\n",
    "The [original version](https://github.com/aimalz/qp) consistently enforced normalization but had limited functionality, whereas the [new version](https://github.com/LSSTDESC/qp) includes many more parameterizations whose usage is \"at your own risk\" in terms of possibly violating normalization.\n",
    "We'll use the new version for the sake of speed but evaluating metrics using simplified functions ported from the old version due to a (hopefully transient) bug in the handling of large sets of posteriors.\n",
    "The first step is to get both the true posteriors and the approximations evaluated on the same grid of redshifts.\n",
    "\n",
    "_This content is adapted from the [qp demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/demo.ipynb), written by Alex Malz (GCCL@RUB), Phil Marshall (SLAC), and Eric Charles (SLAC), and [qp metrics demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/kld.ipynb), written by Alex Malz (GCCL@RUB) and Phil Marshall (SLAC)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WARGQ6Gh-k_"
   },
   "outputs": [],
   "source": [
    "# P = qp.Ensemble(qp.interp, data=dict(xvals=grid.reshape(grid.shape[0]), yvals=posteriors_test))\n",
    "Q = qp.Ensemble(qp.interp, data=dict(xvals=y_grid.reshape(y_grid.shape[0]), yvals=cde_test))\n",
    "Q_bias = qp.Ensemble(qp.interp, data=dict(xvals=y_grid.reshape(y_grid_bias.shape[0]), yvals=cde_test_bias))\n",
    "grid, approx_pdf_on_grid = Q.gridded(grid)\n",
    "grid, approx_pdf_on_grid_bias = Q_bias.gridded(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioNxxbaDoDEm"
   },
   "source": [
    "The Kullback Leibler Divergence (KLD)\n",
    "\\begin{equation}\n",
    "KLD = \\int_{-\\infty}^{\\infty} p(z | \\vec{d}) \\log\\left[\\frac{p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)}{p(z | \\vec{d})}\\right] dz\n",
    "\\end{equation}\n",
    "is a directional measure of how much information is lost by using the estimated $p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)$ instead of the true $p(z | \\vec{d})$.\n",
    "We want the KLD for each galaxy to be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "J-J7YdTtLKPf",
    "outputId": "6d4c71d2-ec4c-4e68-b075-5b3ccd8382db"
   },
   "outputs": [],
   "source": [
    "KLDs = np.array([qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1])) for p, q in zip(posteriors_test, approx_pdf_on_grid)])\n",
    "KLDs_bias = np.array([qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1])) for p, q in zip(posteriors_test, approx_pdf_on_grid_bias)])\n",
    "\n",
    "plt.hist(np.log(KLDs), alpha=0.5, bins=100, label='representative', density=True)\n",
    "plt.hist(np.log(KLDs_bias), alpha=0.5, bins=100, label='biased', density=True)\n",
    "plt.xlabel('KLD')\n",
    "plt.legend()\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR96wyRGPerk"
   },
   "source": [
    "The root-mean-square-error (RMSE) is a symmetric measure commonly used to compare 1D functions.\n",
    "**TODO: write it out?** Similarly, a lower value corresponds to a more closely approximating posterior PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "lrtkIQZXwlMZ",
    "outputId": "86de88d2-193e-411a-ea15-937400ca5c2d"
   },
   "outputs": [],
   "source": [
    "RMSEs = np.array([qp.metrics.quick_rmse(p, q, N=granularity) for p, q in zip(posteriors_test, approx_pdf_on_grid)])\n",
    "RMSEs_bias = np.array([qp.metrics.quick_rmse(p, q, N=granularity) for p, q in zip(posteriors_test, approx_pdf_on_grid_bias)])\n",
    "\n",
    "plt.hist(np.log(RMSEs), alpha=0.5, bins=100, label='representative', density=True)\n",
    "plt.hist(np.log(RMSEs_bias), alpha=0.5, bins=100, label='biased', density=True)\n",
    "plt.xlabel('RMSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTwhvAxUxVnf"
   },
   "source": [
    "**Problem 3g**\n",
    "\n",
    "Apply and visualize the local metrics of [Zhao, Dalmasso, Izbicki & Lee, 2021](https://arxiv.org/abs/2102.10473), or any other metrics not included in this demo; the [cde-diagnostics tutorial](https://github.com/zhao-david/CDE-diagnostics/blob/main/tutorial/tutorial-cde-diagnostics.ipynb), written by David Zhao (CMU), may be a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIwztiqPpgMU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lZsWWSsss9gd"
   ],
   "name": "data challenge prototype",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "DSFP (Python 3)",
   "language": "python",
   "name": "dsfp_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
